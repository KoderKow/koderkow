---
title: tidymodels, rpart, and pengnuins
author: Kyle
date: '2020-08-17'
slug: tidymodels-rpart-and-pengnuins
categories:
  - Programming
tags:
  - Machine Learning
  - R
cover: /img/post_cover/tidymodels_rpart_penguins.png
---

I have seen posts on twitter about the `pegnuins` dataset from the `{palmerpenguins}` package. There has been interest within myself to test some `{tidymodels}` flows on it. As a self labeled "novice data scientist" I have decided to spend my free time on tree models. So this post will be quickly going over the tidymodels process and seeing how well we can predict the different classes of penguins using a simple decision tree!

---

```{r,warning=FALSE, message=FALSE}
library(palmerpenguins)
library(tidymodels)
library(skimr)
library(naniar)
library(vip)
library(rpart.plot)
```

```{r}
penguins
```

```{r}
skim(penguins)
```

## Addressing missing data

```{r}
naniar::gg_miss_upset(penguins)
```

Exploring the data itself will be a separate blog post. So I am opting to remove all rows containing missing data. This will remove a total of 11 rows.

# Split the data

We will set a seed for reproducability. This means when we randomly split the data (80% to training, 20% to testing) that the results of the splits and upcoming cross validation folds will be the same for everyone who uses the same seed. It helps control the randomness.

```{r}
set.seed(1996)

data_split <- 
  penguins %>% 
  drop_na() %>% 
  rsample::initial_split(
  data = ,
  prop = 0.8,
  strata = species
)

train_data <- training(data_split)
test_data <- testing(data_split)
```

I am choosing 5 folds because the `train_data` only has `r nrow(train_data)` rows of data, and this will allow the code to run faster. With a lower $k$ in cross validation, this generally means higher bias and lower variance because each fold will have more training training data. To help address the high bias, I will repeat the 5 fold process three times. In total I will have 15 folds to optimize my models parameters on.

```{r}
penguin_folds <- vfold_cv(
  data = train_data,
  v = 5,
  repeats = 3,
  strata = species
)

penguin_folds
```

Recipes in `{tidymodels}` is a description of what steps should be applied to a data set for model fitting. Here I am fitting a formula on the `train_data`. The `add_role(...)` function changes the role of a variable in the recipe. The `new_role` parameter can take anything we would like. There are key roles like `outcome` and `predictor`. Since I do not want to use the variable `year`, I will assign a random role of `"dont_use"` so that it will not be used during the model fitting process. Since I did not remove this column before hand and in the formula I specied `species ~ .`, that means every column. So this will avoid using that column.

```{r}
my_recipe <-
  recipe(species ~ ., data = train_data) %>% 
  add_role(year, new_role = "dont_use")

my_recipe
```

We can see our model will have one outcome, `species`, and will use 7 predictors to generate that outcome.

rpart models have 3 parameters that can be optimized.

1. **cost_complexity**: This is a parameter that helps "snips" or removes unnesseccary splits of the tree by using a formula (not showing the formula) where the cost complexity value helps determine if the split improves the model all around.
2. **tree_depth**: Sets the maximum depth of any node of the final tree. A higher tree depth can result in overfitting depending on the dataset.
3. **min_n**: The minimum number of observations that must exist in a node before a split in attempted. Setting a low min n can result in overfitting, depending on the dataset.

How can we optimize this? tidymodels makes this extremely easy. First, we must specify the model using tidymodels framework. First we declare the type of model we want, decisition tree. Then the specific engine we want to use, rpart. Finally, the type of output.

Within the model we want to use, decisiton tree, is where we declare what we want to set the parameters to. But, we do not know what the best parameters are. So we will use `tune()` from the `{tune}` package to show the model pipeline we will be optimizing this parameter during the model fitting process.

```{r}
rpart_mod <- 
  decision_tree(
    cost_complexity = tune(),
    tree_depth = tune(),
    min_n = tune()
  ) %>% 
  set_engine("rpart") %>% 
  set_mode("classification")

rpart_mod
```

If you are familiar with your data and know what parameters will work best you can create your own grid. At the moment I am fascinated by these algorithms that tries its best to cover the parameter space given the number of different parameter combinations to test. For this post I will pick 5 so that the model training doesnt take too long. Each fold (we have 15) will be trained 5 times using the different combination of parameters that are generated using `grid_latin_hypercube()`. 

```{r}
rpart_grid <-
  grid_latin_hypercube(
  cost_complexity(),
  tree_depth(),
  min_n(),
  size = 5
  )

rpart_grid
```

A workflow is a container object that aggregates information required to fit and predict from a model. Here I will add our model to fit `rpart_mod` and the recipe we created `my_recipe`.

```{r}
rpart_wf <-
  workflow() %>%
  add_model(rpart_mod) %>%
  add_recipe(my_recipe)

rpart_wf
```

I will now use that workflow process to feed into `tune_grid()`. This function will compute a set of performance metrics (since we are doing classifcation we are looking at ROC AUC) for each of tuning parameter sets on each fold. For classification ROC AUC is used for evaluating models; aside from accuracy it takes sensitivity and specificity into account. Generally, ROC AUC is for a binary output. Here we have three so the method from [Hand, Till, (2001)](https://link.springer.com/article/10.1023/A:1010920819831) will be used to compute the multi class ROC AUC. This will result in 15 rows, a row per fold.

If we do not specify the `metrics` parameter `tune_grid()` will return multiple different evaluation metrics. To simplify the process and we know what metric we want we will specify it.

```{r, message=FALSE, warning=FALSE}
tree_res <- 
  rpart_wf %>% 
  tune_grid(
    resamples = penguin_folds,
    grid = rpart_grid,
    metrics = metric_set(roc_auc)
    )

tree_res
```

`collect_metrics()` will summarize the results per fold and show the results for each tuning parameter. Below shows each tuning parameter combination and its average ROC AUC average per fold. Since I specified a metric in the tune_grid this only shows ROC AUC. If I did not specify `metrics` then we would get back ROC AUC and Accuracy.

```{r}
tree_res %>% 
  collect_metrics() 
```

`show_best("roc_auc")` will arrange our metrics by the highest average.

```{r}
tree_res %>%
  show_best()
```

To select the best performing parameters we can select them by using `select_best()`.

```{r}
best_rpart_params <-
  tree_res %>%
  select_best()

best_rpart_params
```

Now that we have our "best" parameters for our model we can update our workflow.

```{r}
final_wf <- 
  rpart_wf %>% 
  finalize_workflow(best_rpart_params)
```

Using our final workflow we can fit the model using the training data.

```{r}
rpart_fit <- 
  final_wf %>%
  fit(data = train_data) 

rpart_fit 
```

To extract the model from the workflow I will use `pull_workflow_fit()`.

```{r}
best_rpart <-
  rpart_fit %>% 
  pull_workflow_fit()

best_rpart
```

With this model object I can creating variable importance plots using the `{vip}` package.

```{r}
vip(best_rpart)
```

The nice thing about rpart is that sense it is a singular decision tree we can easily visualize it. I will use the `{rpart.plot}` package for this visualization.

```{r, warning=FALSE, message=FALSE}
rpart.plot(
  x =  best_rpart$fit,
  yesno = 2,
  type = 0,
  extra = 0
  )
```

Finally, we can evaluate our final fit against the test data. Using the workflow and piping it into `last_fit(data_split)`, this process will take everything we have done and produce these results.

```{r}
final_fit <- 
  final_wf %>%
  last_fit(data_split) 

final_fit
```

```{r}
final_fit %>%
  collect_metrics()
```

95% ROC AUC! Great results for a simple tree algorithm. I can visual the ROC AUC curves using the `roc_curve` function. Since this is a multiclass output and the ROC AUC is calculated as an average of the pairwise ROC AUC per class, there will have to be a plot per class.

```{r}
penguin_roc_curves <-
  final_fit %>%
  collect_predictions() %>% 
  roc_curve(species, .pred_Adelie:.pred_Gentoo)

penguin_roc_curves
```

```{r}
autoplot(penguin_roc_curves)
```

---

# References

## Packages

```{r, echo=FALSE}
wanted_pkgs <- c("palmerpenguins", "tidymodels", "skimr", "naniar", "vip", "rpart.plot")
sess <- sessionInfo()
sess$otherPkgs <- sess$otherPkgs[names(sess$otherPkgs) %in% wanted_pkgs]
cite <- report::cite_packages(sess)
names(cite) <- ""
knitr::kable(cite)
```

## Papers

Hand, Till (2001). "A Simple Generalisation of the Area Under the ROC Curve for Multiple Class Classification Problems". Machine Learning. Vol 45, Iss 2, pp 171-186.